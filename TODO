Workflow:
    - Get query
    - Break query up into sub-queries to send
    - fetch data for sub-queries
    - combine data into result
    - return result

Notes To sort out:
    - handle missing data due to broken instance in cluster
    - remove need for always aggregating data (even if it isn't ever looked at)
    - dynamic discovery of prometheus hosts (adding/removing etc. is "safe"!)
    - metrics on query performance and cluster health (as we're querying things!)
    - 


Do we need?
    - affinity?
        -- if we don't then graphs might move around as the times within the cluster
            won't exactly match up-- if we do have affinity config is complicated
            as to *breaking* the affinity
    - 

Use-cases to cover:
    - Mutiple redundant prometheus clusters
        -- pick which timeseries to serve (most data, best spread data, etc.)
        -- merge datasets together (all unique data points?)
    - separate shards of prometheus
        -- merge 2 data from N clusters into a single result
            -- this requires parsing the query to see if the top level query expr
                is an aggregation, if so, then we need to aggregate the vectors
                across the clusters that we saw

Phases:
    - Read-Only
        -- static host list
        -- single query to each cluster
        -- merge data within cluster
        -- dynamic host list
    - Read-Write
        -- Presumably a delete would be successful if all hosts in the target return GOOD


TODO:
    - look into replacing promclient structs with prometheus ones
    - configs
    - finish arg implementation of HTTP API
    - add "native" step to prometheus API
    - 
